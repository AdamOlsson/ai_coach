from torchvision.datasets.video_utils import VideoClips
import torchvision, cv2, torch
import numpy as np

from os import listdir, mkdir, makedirs
from os.path import isfile, join, splitext, exists
from shutil import rmtree
import sys
"""
The script applies the sliding window method for a given video. However, each sample 
that is generated by the sliding window is divided into subclips due to memory constraints.
All subclips for a sample are located in a single directory
Usage:
python util/preprocessing/sliding_window.py
"""


def rotate(image, **kwargs):
    image_center = tuple(np.array(image.shape[1::-1]) / 2)
    rot_mat = cv2.getRotationMatrix2D(image_center, kwargs["rotation"], 1.0)
    result = cv2.warpAffine(image, rot_mat, image.shape[1::-1], flags=cv2.INTER_LINEAR)
    return result

def scaleAndCrop(image, **kwargs):
    scale_factor = kwargs["scale_factor"]
    crop_scale_y, crop_scale_x = kwargs["crop_scale"][0], kwargs["crop_scale"][1]

    resized_image = cv2.resize(image, None, fx=scale_factor, fy=scale_factor)

    old_w, old_h = image.shape[1], image.shape[0]
    new_w, new_h = resized_image.shape[1], resized_image.shape[0]

    if scale_factor > 1:
        # upscaled, need to crop
        top_left_y = int(crop_scale_y*(new_h - old_h))
        top_left_x = int(crop_scale_x*(new_w - old_w))

        image[:] = resized_image[top_left_y:top_left_y+old_h, top_left_x:top_left_x+old_w]

        return image

    else:
        # downscaled, need to put into image
        top_left_y = int(crop_scale_y*(old_h - new_h))
        top_left_x = int(crop_scale_x*(old_w - new_w))

        image.fill(0)
        image[top_left_y:top_left_y + new_h, top_left_x:top_left_x + new_w] = resized_image[:]

        return image 

def sliding_window(video_path, save_path, epoch_id, preprocess=[]):
    T = 300

    videoclips = VideoClips([video_path], clip_length_in_frames=T, frames_between_clips=1)

    filenames = []
    rotation = 0
    sample_count = -1
    sample_dir = ""
    sample_id = ""
    for i in range(len(videoclips)):

        sample_count = hash(str(sample_count + 1 + epoch_id*(len(videoclips)/T))) % ((sys.maxsize + 1) * 2)

        # create new preprocess values
        rnd = np.random.uniform(-1,1)
        rotation = 45 * rnd
        scale_factor = np.random.uniform(0.4, 1.4)
        crop_scale_y = np.random.uniform(0,1)
        crop_scale_x = np.random.uniform(0,1)

        # Preprocess
        clip,_,_, _ = videoclips.get_clip(i)

        clip = clip.numpy()

        for f in range(len(clip)):
            for p in preprocess:
                clip[f] = p(clip[f], rotation=rotation, scale_factor=scale_factor, crop_scale=(crop_scale_y, crop_scale_x))
        
        clip = torch.tensor(clip)

        # Save
        filename = "{}.mp4".format(hex(sample_count))
        filepath = join(save_path, filename)
        torchvision.io.write_video(filepath, clip, 30)
        filenames.append(filename)
        print("{}, {}, {}/{}".format(filepath, epoch_id, i, len(videoclips)))


    return filenames


def main(input_dir, output_dir, dataset_name, epochs=1):
    videos = [f for f in listdir(input_dir) if isfile(join(input_dir, f))]
    labels = [splitext(f)[0] for f in videos ]

    working_dir = join(output_dir, dataset_name)


    samples_dir = join(working_dir, "samples")
    annotations_file = join(working_dir, "annotations.csv")

    partial_run = False
    if not partial_run: 
        # partial run, if I run epochs at separate occations.
        # DONT FORGET TO CHANGE EPOCH VALUE
        if exists(working_dir):
           rmtree(working_dir)
 
        makedirs(samples_dir)
 
        # setup csv
        with open(annotations_file, "a") as f:
           f.write("# filename,label\n")

    preprocess = [scaleAndCrop, rotate] # list of functions
    for e in range(epochs):
        for v, l in zip(videos, labels):
            class_dir = join(samples_dir, l)
            
            if not exists(class_dir):
                mkdir(class_dir)

            filenames = sliding_window(join(input_dir,v), class_dir, e, preprocess=preprocess) # DONT FORGET TO CHANGE e TO CORRECT LOOP ITERATION

            # create csv from filenames
            with open(annotations_file, "a") as f:
                for name in filenames:
                    f.write("samples/{},{}\n".format(join(l,name), l))


if __name__ == "__main__":
    input_dir = "../datasets/weightlifting/sliding_window/full_videos"
    output_dir = "../datasets/weightlifting/sliding_window"
    main(input_dir, output_dir, "slided")